{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anthony/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras import backend as k\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, TimeDistributed, Flatten\n",
    "from tensorflows.keras.models import load_model\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")   # download list of stopwords\n",
    "from nltk.corpus import stopwords # import stopwords\n",
    "import numpy as np\n",
    "\n",
    "class Sentiment_Analysis:\n",
    "\n",
    "    def load_data(self, directory=\"./data/imdb-reviews\"):\n",
    "        \"\"\"Read data a from given directory.\n",
    "\n",
    "        Directory structure expected:\n",
    "        - data/\n",
    "            - train/\n",
    "                - pos/\n",
    "                - neg/\n",
    "            - test/\n",
    "                - pos/\n",
    "                - neg/\n",
    "    \"\"\"\n",
    "\n",
    "        # returning data and labels in nested dictionaries matching the directory structure\n",
    "        self.data = {}\n",
    "        self.labels = {}\n",
    "\n",
    "        # sub-directories for the features: train, test\n",
    "        for data_type in ['train', 'test']:\n",
    "            self.data[data_type] = {}\n",
    "            self.labels[data_type] = {}\n",
    "\n",
    "            #sub-directories for the sentiments (label): pos, neg\n",
    "            for sentiment in ['pos', 'neg']:\n",
    "                self.data[data_type][sentiment] = []\n",
    "                self.labels[data_type][sentiment] = []\n",
    "\n",
    "                # Fetching the list of files for this sentiment\n",
    "                path = os.path.join(directory, data_type, sentiment, '*.txt')\n",
    "                files = glob.glob(path)\n",
    "\n",
    "                # Read text data and labels and populate dictionaries\n",
    "                for file in files:\n",
    "                    with open(file) as text:\n",
    "                        self.data[data_type][sentiment].append(text.read())\n",
    "                        self.labels[data_type][sentiment].append(sentiment)\n",
    "\n",
    "                #checking for discrepencies\n",
    "                assert len(self.data[data_type][sentiment]) == len(self.labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "\n",
    "\n",
    "\n",
    "    def join_data(self):\n",
    "        \"\"\"\n",
    "        Prepares training and test sets from the loaded data - combining\n",
    "        Returns unified training data, test data, training labels and test labels\n",
    "        \"\"\"\n",
    "\n",
    "        #Combinining positive and negative reviews and labels\n",
    "        self.data_train = self.data['train']['pos'] + self.data['train']['neg']\n",
    "        self.data_test = self.data['test']['pos'] + self.data['test']['neg']\n",
    "        self.labels_train = self.labels['train']['pos'] + self.labels['train']['neg']\n",
    "        self.labels_test = self.labels['test']['pos'] + self.labels['test']['neg']\n",
    "\n",
    "        #Shuffling reviews and corresponding labels within training and test sets\n",
    "        self.data_train, self.labels_train = shuffle(self.data_train, self.labels_train)\n",
    "        self.data_test, self.labels_test = shuffle(self.data_test, self.labels_test)\n",
    "\n",
    "    def tokenize(self,text):\n",
    "        \"\"\"\n",
    "        transforms raw text into a sequence of words\n",
    "        input:\n",
    "        text unprocessed\n",
    "        output\n",
    "        final list of words\n",
    "        \"\"\"\n",
    "        #removing html tags\n",
    "        text = BeautifulSoup(text, \"html5lib\").get_text()\n",
    "        # convert to lower case\n",
    "        text = text.lower()\n",
    "        #remove non letters characters\n",
    "        text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "        #splitting\n",
    "        words = text.split()\n",
    "        #remove stopwords\n",
    "        words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "\n",
    "        # Return the final list of words\n",
    "        return words\n",
    "\n",
    "    def encode_data(self):\n",
    "        \"\"\"\n",
    "        Tokenize and encode data into a list of integers.\n",
    "        input:\n",
    "            training data\n",
    "            training labels\n",
    "        output:\n",
    "            encoded trainnig features\n",
    "            encoded training labels\n",
    "        \"\"\"\n",
    "\n",
    "        #Tokenizing\n",
    "        training_features = [self.tokenize(text) for text in self.data_train]\n",
    "\n",
    "        # Initialize word2id and label2id dictionaries that will be used\n",
    "        # to encode words and labels\n",
    "        self.word2id = dict()\n",
    "        self.label2id = dict()\n",
    "        # maximum number of words in a sentence\n",
    "        self.max_words = 0\n",
    "\n",
    "        # Constructing a word2id dict\n",
    "        for sentence in training_features:\n",
    "            for word in sentence:\n",
    "                # Adding words to the dictionnay if they are not referenced\n",
    "                if word not in self.word2id:\n",
    "                    self.word2id[word] = len(self.word2id)\n",
    "        # When the length of the sentence is greater than max_words, updating max_words\n",
    "        if len(sentence) > self.max_words:\n",
    "            self.max_words = len(sentence)\n",
    "\n",
    "        #Building label2id and id2label dictionaries - using a set to drop duplicates\n",
    "        self.label2id = {label: i for i, label in enumerate(set(self.labels_train))}\n",
    "        self.id2label = {key: value for value, key in self.label2id.items()}\n",
    "\n",
    "        # Encode features and labels\n",
    "        self.encoded_features = [[self.word2id[word] for word in sentence] for sentence in training_features]\n",
    "        self.encoded_labels = [self.label2id[label] for label in self.labels_train]\n",
    "\n",
    "        # Padding the encoded features so they are all the same length\n",
    "        self.encoded_features = pad_sequences(self.encoded_features, self.max_words)\n",
    "\n",
    "        # Converting the encoded labels to a matrix\n",
    "        self.encoded_labels = to_categorical(self.encoded_labels, num_classes=len(self.label2id))\n",
    "\n",
    "\n",
    "    def build_network(self):\n",
    "        \"\"\"\n",
    "        Builds a network sequentially\n",
    "        \"\"\"\n",
    "        #model stucture\n",
    "        embedding_size = 64\n",
    "        self.model = Sequential()\n",
    "        #Embedding layer\n",
    "        self.model.add(Embedding(input_dim=len(self.word2id),\n",
    "                                 output_dim = embedding_size,\n",
    "                                 input_length=self.max_words))\n",
    "        #Mapping with a Bidirectional wrapper and a LSTM layer\n",
    "        self.model.add(LSTM(100, return_sequences=True))\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(LSTM(100, return_sequences=True))\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(LSTM(100))\n",
    "        self.model.add(Dropout(0.5))\n",
    "        #Dense layer with a sigmoid activation to\n",
    "        self.model.add(Dense(len(self.label2id), activation='sigmoid'))\n",
    "\n",
    "\n",
    "        #hyperparameters\n",
    "        learning_rate = 0.003\n",
    "\n",
    "        #compiling model\n",
    "        self.model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=Adam(learning_rate),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    def train(self, batch_size = 32, epochs = 0):\n",
    "        \"\"\"\n",
    "        Trains the network on the training dataset\n",
    "        input:\n",
    "        array of training features\n",
    "        array of matching tags\n",
    "        batch_size\n",
    "        num of epochs\n",
    "        return:\n",
    "        trained classifier\n",
    "        \"\"\"\n",
    "\n",
    "        # #Reserve some training data for validation\n",
    "        # validation_features, validation_labels = \\\n",
    "        # self.encoded_features[:batch_size],self.encoded_labels[:batch_size]\n",
    "        # train_features, train_labels = \\\n",
    "        # self.encoded_features[batch_size:],self.encoded_labels[batch_size:]\n",
    "\n",
    "        #Train the model\n",
    "        print(\"**Training in Progress**\")\n",
    "        self.model.fit(self.encoded_features, self.encoded_labels,\n",
    "                       validation_split=0.2, verbose = 1,\n",
    "                       batch_size=batch_size, epochs= epochs)\n",
    "\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Evaluates the accuracy of the network on the test set\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_test = [self.tokenize(text) for text in self.data_test]\n",
    "        self.encoded_test_features = [[self.word2id.get(\"word\", 0) for word in sentence] for sentence in self.data_test]\n",
    "        self.encoded_test_labels = [self.label2id[label] for label in self.labels_test]\n",
    "        # Padding the encoded features so they are all the same length\n",
    "        self.encoded_test_features = pad_sequences(self.encoded_test_features, self.max_words)\n",
    "\n",
    "        # Converting the encoded labels to a matrix\n",
    "        self.encoded_test_labels = to_categorical(self.encoded_test_labels, num_classes=len(self.label2id))\n",
    "        # returns loss and other metrics specified in model.compile()\n",
    "        scores = self.model.evaluate(self.encoded_test_features, self.encoded_test_labels, verbose=0)\n",
    "        # scores[1] should correspond to accuracy as we passed the metrics =['accuracy']\n",
    "        print(\"Test accuracy:\", scores[1])\n",
    "\n",
    "\n",
    "\n",
    "    def load(self, file=\"Sentiment_Analysis.h5\"):\n",
    "        \"\"\"\n",
    "        Load the trained network from cache or build, train, test and save the network\n",
    "        to the cache\n",
    "        input:\n",
    "            optional - cache file as a .h5\n",
    "        output:\n",
    "            trained model\n",
    "            optional - cache file as a .h5\n",
    "        \"\"\"\n",
    "\n",
    "        #defining where to store cache files\n",
    "        directory = os.path.join(\"cache\", \"sentiment_analysis\")\n",
    "        #Ensure the cache_directory exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        #Checking for cache file and loading the model\n",
    "        if os.path.exists(os.path.join(directory, file)):\n",
    "            self.model = load_model(os.path.join(directory, file))\n",
    "            print(\"**Model loaded from\", file)\n",
    "\n",
    "        #Building, training, testing and saving the model\n",
    "        else:\n",
    "            print(\"**Loading Data**\")\n",
    "            self.load_data()\n",
    "            self.join_data()\n",
    "            print(\"**Encoding Data**\")\n",
    "            self.encode_data()\n",
    "            print(\"**Building Network**\")\n",
    "            self.build_network()\n",
    "            print(\"**Training Network**\")\n",
    "            self.train()\n",
    "            print(\"**Testing Network**\")\n",
    "            self.test()\n",
    "            self.model.save(os.path.join(directory, file))\n",
    "            print(\"**Model saved\", os.path.join(directory, file), \"**\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_sentiment(self,text):\n",
    "        \"\"\"\n",
    "        Predicts the sentiment of a text\n",
    "        input:\n",
    "        text as a string\n",
    "        return:\n",
    "        keras model predict?????\n",
    "        \"\"\"\n",
    "        # Prepare the text\n",
    "        prepared_text = self.tokenize(text)\n",
    "        print(prepared_text)\n",
    "        prepared_text = [[self.word2id.get(\"word\", 0) for word in sentence] for sentence in prepared_text]\n",
    "        prepared_text = pad_sequences(prepared_text, self.max_words)\n",
    "        print(prepared_text)\n",
    "\n",
    "        # Predict sentiment\n",
    "        prediction = self.model.predict_classes(prepared_text, verbose = 0)\n",
    "        print(prediction)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Sentiment_Analysis()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.encode_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.build_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Training in Progress**\n",
      "Train on 8 samples, validate on 2 samples\n"
     ]
    }
   ],
   "source": [
    "x.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "x.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['awesome', 'sauce']\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "x.get_sentiment(\"awesome sauce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Loading Data**\n",
      "**Encoding Data**\n",
      "**Building Network**\n",
      "**Training Network**\n",
      "**Training in Progress**\n",
      "Train on 8 samples, validate on 2 samples\n",
      "**Testing Network**\n",
      "Test accuracy: 0.5\n",
      "**Model saved cache/sentiment_analysis/Sentiment_Analysis.h5 **\n"
     ]
    }
   ],
   "source": [
    "x.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-26f52acbda81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-201-893e07549fb2>\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;31m#Checking for cache file and loading the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"**Model loaded from\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "x.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
